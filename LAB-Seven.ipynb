{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baking-mercy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/goldman1/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "import re\n",
    "import nltk\n",
    "import math\n",
    "import pandas as pd\n",
    "nltk.download('stopwords')\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "taken-following",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "static-eugene",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCSV(fileName):\n",
    "    rows  =  []\n",
    "    result = []\n",
    "    with open(fileName, 'r',encoding=\"ISO-8859-1\") as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        next(csvreader)\n",
    "        for row in csvreader:\n",
    "            rows.append(row[:2])\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "searching-branch",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanAndTokenizeSentence(sentence):\n",
    "        text = []\n",
    "        sentence = sentence.strip().lower().split()\n",
    "        for word in sentence:\n",
    "            text.append(\"\".join(w for w in re.findall(\"([a-z\\-.']+)\", word)))\n",
    "        text = \" \".join(text)\n",
    "        for key, val in dict({'“': '\"', '”': '\"', '’': \"'\", '--': ','}).items():\n",
    "            text = text.replace(key,val)\n",
    "        \n",
    "        cleanWords = []\n",
    "        for word in nltk.tokenize.word_tokenize(text):\n",
    "            if word not in stop_words and word != '.' * len(word):\n",
    "                cleanWords.append(word)\n",
    "        return cleanWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "changed-japan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitTrainAndTestData(rows, percent = 0.9,seed = 30):    \n",
    "    random.seed(seed)\n",
    "    rows.sort()\n",
    "    trainSize = int(len(rows) * percent)\n",
    "    random.shuffle(rows)\n",
    "    return rows[:trainSize] , rows[trainSize:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "amateur-validity",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.tfValues = defaultdict(lambda: defaultdict(int))\n",
    "        self.wordTfValues = defaultdict(lambda: defaultdict(int))\n",
    "        self.nDocumentsContainingWord = defaultdict(lambda: defaultdict(int))\n",
    "        self.trainCount = defaultdict(lambda: defaultdict(int));\n",
    "        self.totalWordsinClass = defaultdict(int);\n",
    "        self.classCount = defaultdict(int)\n",
    "        self.trainOnce = False\n",
    "    \n",
    "    def train(self):\n",
    "        if self.trainOnce == True:\n",
    "            return\n",
    "        self.trainOnce = True \n",
    "        for i in range(len(self.data)):\n",
    "            self.data[i][1] = cleanAndTokenizeSentence(self.data[i][1])\n",
    "            \n",
    "            termVocab = set(self.data[i][1])\n",
    "            termVocabSize = len(termVocab)\n",
    "            termTfValues = {}\n",
    "            \n",
    "            for word in termVocab:\n",
    "                self.nDocumentsContainingWord[word][self.data[i][0]] = self.nDocumentsContainingWord[word][self.data[i][0]]+ 1 \n",
    "                self.nDocumentsContainingWord[word]['all'] = self.nDocumentsContainingWord[word]['all']+ 1\n",
    "                \n",
    "            for word in self.data[i][1]:\n",
    "                termTfValues[word] = termTfValues.get(word, 0) + 1\n",
    "            \n",
    "            for value in termTfValues:\n",
    "                termTfValues[value] = termTfValues[value]/termVocabSize\n",
    "                self.wordTfValues[value][i] = termTfValues[value]\n",
    "            \n",
    "            self.tfValues[i][0] = self.data[i][0]\n",
    "            self.tfValues[i][1] = termTfValues\n",
    "            \n",
    "            self.classCount[self.data[i][0]]+=1\n",
    "            \n",
    "            for word in self.data[i][1]:\n",
    "                self.trainCount[self.data[i][0]][word]  += 1\n",
    "                self.totalWordsinClass[self.data[i][0]] += 1\n",
    "\n",
    "    \n",
    "    def test(self, sentence, tell_all_word_prob = False):\n",
    "        def myPrint(*args):\n",
    "            if tell_all_word_prob:\n",
    "                print(\" \".join(args))\n",
    "            \n",
    "        all_prob = []\n",
    "        myPrint(f'->Test Sentence is:\\n',sentence,'\\n')\n",
    "        sentence = cleanAndTokenizeSentence(sentence)\n",
    "        myPrint(f'Tokens:\\n{sentence}')\n",
    "        \n",
    "        \n",
    "        for key in self.classCount:\n",
    "            myPrint(f\"For class {key}\")\n",
    "            \n",
    "            prob = 1.0\n",
    "            \n",
    "            for token in sentence:\n",
    "                if token not in self.wordTfValues.keys():\n",
    "                    num = 0\n",
    "                else:\n",
    "                    k = 0\n",
    "                    num = 0\n",
    "                    for entry in self.wordTfValues[token]:\n",
    "                        if key != self.tfValues[k][0]:\n",
    "                            continue\n",
    "                        num += entry * math.log10(len(self.data)/self.nDocumentsContainingWord[token]['all'])\n",
    "                        k = k+1\n",
    "                \n",
    "                denum = 0\n",
    "                for i in range(len(self.tfValues)):\n",
    "                    entry = self.tfValues[i]\n",
    "                    if entry[0] != key:\n",
    "                        continue\n",
    "                    for entryWord in entry[1]:\n",
    "                        denum += entry[1][entryWord]*math.log10(len(self.data)/self.nDocumentsContainingWord[entryWord]['all'])\n",
    "             \n",
    "                num += 1                             \n",
    "                denum += len(self.trainCount[key])  \n",
    "                prob *= num / denum\n",
    "                myPrint(\"\\t\",f\"The probability of word ({token}) is {round(num / denum, 4)}\")\n",
    "            prob *= self.classCount[key] / len(self.data)\n",
    "            all_prob.append((prob, key))\n",
    "        all_prob.sort(reverse=True)\n",
    "    \n",
    "        return (all_prob[0][0],all_prob[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "pressing-mandate",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = readCSV(r\"./dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "popular-burner",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = splitTrainAndTestData(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "indirect-irrigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NaiveBayes(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "general-spelling",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "meaning-shape",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior probability of class (ham) is 48.89\n",
      "Prior probability of class (spam) is 51.11\n"
     ]
    }
   ],
   "source": [
    "for key in model.classCount:\n",
    "    print(f\"Prior probability of class ({key}) is {round(model.classCount[key]/len(model.data)*100,2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "developmental-complexity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "->Test Sentence is:\n",
      " Ffffffffff. Alright no way I can meet up with you sooner? \n",
      "\n",
      "Tokens:\n",
      "['ffffffffff', 'alright', 'way', 'meet', 'sooner']\n",
      "For class ham\n",
      "\t The probability of word (ffffffffff) is 0.0014\n",
      "\t The probability of word (alright) is 0.0014\n",
      "\t The probability of word (way) is 0.1163\n",
      "\t The probability of word (meet) is 0.5469\n",
      "\t The probability of word (sooner) is 0.0014\n",
      "For class spam\n",
      "\t The probability of word (ffffffffff) is 0.0012\n",
      "\t The probability of word (alright) is 0.0012\n",
      "\t The probability of word (way) is 0.0012\n",
      "\t The probability of word (meet) is 0.0012\n",
      "\t The probability of word (sooner) is 0.0012\n",
      "\n",
      "\n",
      "The predicted class is ham with probablity 9.419089393554257e-11\n"
     ]
    }
   ],
   "source": [
    "prob, c =  model.test(test[0][1],True)\n",
    "\n",
    "print(\"\\n\\nThe predicted class is \" +  str(c) + \" with probablity \" + str(prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20cb4a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31fd7de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(train)\n",
    "test = pd.DataFrame(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b4a8b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       ham\n",
       "1       ham\n",
       "2      spam\n",
       "3       ham\n",
       "4      spam\n",
       "       ... \n",
       "175    spam\n",
       "176    spam\n",
       "177    spam\n",
       "178    spam\n",
       "179     ham\n",
       "Name: 0, Length: 180, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y = train[0]\n",
    "train_X = train[1]\n",
    "train_X = [' '.join(map(str, l)) for l in train_X]\n",
    "\n",
    "test_Y = test[0]\n",
    "test_X = test[1]\n",
    "test_X = [' '.join(map(str, l)) for l in test_X]\n",
    "train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39f81917",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = TfidfVectorizer()\n",
    "X_train_tf = tf_vectorizer.fit_transform(train_X)\n",
    "\n",
    "X_test_tf = tf_vectorizer.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82bd36f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_bayes_classifier = MultinomialNB()\n",
    "naive_bayes_classifier.fit(X_train_tf, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0e3dd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction is: ham\n"
     ]
    }
   ],
   "source": [
    "prediction = naive_bayes_classifier.predict(tf_vectorizer.transform(['Ffffffffff. Alright no way I can meet up with you sooner?']))\n",
    "\n",
    "print(\"The prediction is: \" + prediction[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
